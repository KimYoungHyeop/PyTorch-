{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_9_2022_02_14.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9gdIjp39iQcRfZT6bV2f3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 고전 모델, 최신 모델 더 배울 것들"
      ],
      "metadata": {
        "id": "HshZjTaQIHKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 지금까지 배운 내용 정리\n",
        "---\n",
        "학습 패러다임으로 시작해서 계산 그래프를 사용해 복잡한 문제를 역전파로 훈련할 수 있는 하나의 모델로 구성하는 방법을 알아보았습니다. 파이토치를 계산 그래프 프레임워크로 소개했습니다.  \n",
        "\n",
        "2장과에서는 NLP의 기본 개념과 언어학을 소개했습니다. 3장에서 활성화 함수, 손실 함수, 지도 학습을 위한 그레이디언트 기반 최적화, 훈련-평가 반복 같은 기초적인 개념을 설명했습니다. 4장에서 피드 포워드 신경망인 다층 퍼셉트론(MNP)과 합성곱 신경망(CNN)으로 예제를 두 개 만들었습니다. 또한 신경망을 안정적으로 훈련하는 데 도움이 되는 L1, L2, 드롭아웃 같은 규제를 어떻게 사용하는지 보았습니다.  \n",
        "MLP는 은닉층에서 n-gram과 유사한 관계를 감지할 수 있지만 효율적이지 않습니다. 반면 CNN은 '파라미터 공유'라는 효율적인 계산 방식으로 이런 부분 구조를 학습합니다.  \n",
        "\n",
        "6장에서는 순환 신경망(RNN)이 적은 파라미터로 시간에 따른 긴 의존성을 감지하는 방법을 알아보았습니다. CNN은 공간에 걸쳐 파라미터를 공유하고 RNN은 시간에 걸쳐 파라미터를 공유합니다. 엘만 RNN, 게이트가 있는 LSTM, GRU까지 3개의 RNN을 알아보았습니다. 또한 RNN을 사용해 예측이나 입력 타임 스텝마다 출력을 예측하는 시퀀스 레이블링 작업에 사용하는 방법을 알아보았습니다. 마지막으로 인코더-디코더 모델을 소개하고 기계 번역같이 조건이 있는 생성 문제를 푸는 데 유용한 시퀀스-투-시퀀스 모델을 배웠습니다. 이런 주제들을 다룰 때 파이토치로 엔드 투 엔드 예제를 구성했습니다."
      ],
      "metadata": {
        "id": "M8Qyt6hDIIMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전통적인 NLP 주제\n",
        "---"
      ],
      "metadata": {
        "id": "On6kQFLpIIOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <대화 및 상호작용 시스템>  \n",
        "\n",
        "컴퓨터와 사람이 원할하게 대화하는 것은 컴퓨터 분야의 중요한 목표이며 튜링 테스트와 뢰브너 상에 영감을 주었습니다. 대화식 시스템과 상호작용 시스템 분야는 다양한 연구 결실을 보았습니다. 아마존의 알렉사, 애플의 시리, 구글의 어시스턴트와 같은 제품의 성공이 증거입니다. 대화식 시스템(무엇이든 물어볼 수 있는)은 개방형(항공 예약과 차량 내비게이션 같은)이거나 폐쇄형일 수 있습니다.  \n",
        "\n",
        "이 분야의 중요한 연구 주제는 다음과 같습니다.  \n",
        "대화 행위, 대화 문맥, 대화 상태를 어떻게 모델링하는가?  \n",
        "\n",
        "*   대화 행위, 대화 문맥, 대화 상태를 어떻게 모델링하는가? \n",
        "*   (음성, 영상, 텍스트를 입력으로 받는) 멀티모달 대화 시스템을 어떻게 만드는가?\n",
        "*   어떻게 시스템이 사용자의 의도를 인식할 수 있는가?  \n",
        "*   어떻게 사용자의 기호를 모델링하여 사용자에게 잘 맞는 응답을 생성할 수 있는가?  \n",
        "*   어떻게 더 인간적인 응답을 만들 수 있는가?  \n",
        "\n",
        "최근에는 대화식 상용 시스템에서 '음(umm)' 이나 '어(uh)' 같은 추임새를 사용해 시스템이 덜 로봇처럼 보이게 합니다."
      ],
      "metadata": {
        "id": "ZT4plY7fIIQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <담화 분석>\n",
        "\n",
        "담화 분석은 텍스트 문저의 부분-전체 특징을 이해하는 것입니다. 예를 들어 담화 파싱 작업은 두 문장의 문맥적인 연관성을 이해하는 작업입니다.  \n",
        "밑에 보이는 표에 PDTB(Penn Discourse Treebank)의 예가 몇 가지 있습니다.  \n",
        "CoNLL 2015의 단순 담화 분석(Shallow Discourse Processing) 작업의 예입니다.  \n",
        "\n",
        "\n",
        "|예시|담화 관계|\n",
        "|------|---|\n",
        "|GM officials want to get their strategy to reduce capacity and the workforce in place <U>before</U> **those talks begin.**|Temporal.Asynchronous. Precedence|\n",
        "|But that ghost wouldn't settle for words, he wanted money and people-lots. <U>So</U> **Mr. Carter formed three new Army divisions and gave them to a new bureaucracy in Tampa called the Rapid Deployment Force.**|Contingency.Cause.Result|\n",
        "|**The Arabs had merely oil.** <U>Implicit=while</U> These farmers may have a grip on the world's very heart|Comparison.Contrast|"
      ],
      "metadata": {
        "id": "PFPPcNs1IHM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "또한, 담화 이해는 ***대용어 복원(anaphora resolution)*** 과 ***환유 감지(metonymy detection)*** 같은 문제를 해결하는 작업을 포함합니다. 대용어 복원은 대명사가 참조하는 해당 개체(entity)를 찾습니다. 밑에 예시에서 알 수 있듯이 이는 복잡한 문제일 수 있습니다."
      ],
      "metadata": {
        "id": "H57y8LqxIHPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) The dog chewed the bone. It was delicious.  \n",
        "(b) The dog chewed the bone. It was a hot day.  \n",
        "(c) Nia drank a tall glass of beer. It was chipped.  \n",
        "(d) Nia drank a tall glass of beer. It was bubbly.  \n",
        "\n",
        "설명 : 대용어 복원에서 일어날 수 있는 몇 가지 문제. (a)에서 'It'은 dog 또는 bone을 의미하나요? (b)에서 'It'은 각각 glass와 beer를 의미합니다. 유리잔보다 맥주에 거품이 있을 가능성이 높음을 아는 것은 참조 대상을 찾는 데 중요한 역할을 합니다(선택적 선호도selectinal preference).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UuduvIA3IHR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "참조 대상이 다음 예에서처럼 환유어(metonym)이기도 합니다.  \n",
        "\n",
        "<U>Beijing</U> imposed trade tariffs in response to tariffs on Chinese goods.  \n",
        "\n",
        "여기서 'Beijing'은 지역이 아니라 중국의 정부를 의미합니다. 참조 대상을 잘 파악하려면 떄로는 기반 지식이 필요합니다."
      ],
      "metadata": {
        "id": "EOObdfZOIHUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <정보 추출과 텍스트 마이닝>  \n",
        "산업 분야에서는 정보 추출과 관련된 문제를 흔히 맞닥뜨립니다. \n",
        "\n",
        "* 텍스트에서 개체명(사람 이름, 제품 이름 등), 이벤트, 관계를 어떻게 추출하나요?  \n",
        "* 텍스트에서 언급된 개체를 언급된 개체를 지식 데이터베이스에 있는 항목에 어떻게 매핑하나요?(개체명 검색, 개체명 연결, 슬롯 필링이라고도 부릅니다)  \n",
        "* 지식 베이스를 구축하고 관리하는 방법은 무엇인가요?  \n",
        "\n",
        "이런 질문은 정보 추출 연구가 다양한 맥락에서 해결하려는 작업입니다."
      ],
      "metadata": {
        "id": "HGyyMmg9IHV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <문서 분석과 문서 추출>  \n",
        "대량의 문서를 이해하는 작업은 산업 분야 NLP의 또 다른 문제입니다. \n",
        "* 문서에서 어떻게 주제를 뽑나요?(토픽 모델링)  \n",
        "* 어떻게 문서를 지능적으로 인덱스하고 검색할 수 있나요?  \n",
        "* 어떻게 검색 쿼리를 이해할 수 있나요?(쿼리 파싱)  \n",
        "* 많은 문서를 어떻게 요약할 수 있나요?  \n",
        "\n",
        "NLP 기술의 범주와 적용 분야는 광범위합니다. 실제로 NLP 구조적이지 않거나 부분적으로만 구조를 갖춘 데이터가 있는 어느 곳에나 적용될 수 있습니다.  \n",
        "예를 들어 Dill 등의 논문에서는 자연어 파싱 기술을 적용해서 단백질 접힘(protein folding)을 설명했습니다."
      ],
      "metadata": {
        "id": "IRT89avJIHXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 최신 NLP 모델(2018년 가을 기준)"
      ],
      "metadata": {
        "id": "_O19RfGFIHbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **<전통적인 NLP 연구에서 딥러닝 패러다임으로의 전환>**  \n",
        ">\n",
        ">NLP 분야의 역사는 수십 년이지만 딥러닝 분야는 몇 년 되지 않았습니다. 많은 혁신이 딥러닝(미분 가능한 학습) 패러다임 하에서 전통적인 방식과 작업을 검토하는 것 같습니다. 고전 NLP 논문을 읽을 때 저자가 무엇을 학습하려는지 질문하는 것이 좋습니다.  \n",
        "\n",
        "> **<모델의 조합>**\n",
        ">\n",
        ">MLP, CNN, 시퀀스 모델, 시퀀스-투-시퀀스 모델, 어텐션 기반 모델과 같은 NLP에 사용하는 여러 종류의 딥러닝 구조를 소개했는데요 논문에서는 주어진 문제를 해결하는 데 여러 가지 모델을 조합하는 경향이 있습니다.  \n",
        "예를 들어 단어의 문자에 대한 CNN을 만든 다음 LSTM을 연결합니다. 마지막으로 MLP를 사용해 LSTM의 출력을 분류합니다. 작업에 따라 여러 구조를 조합할 수 있다는 점은 딥러닝의 가장 강력한 기능이며 작업을 성공적으로 수행하는 열쇠입니다.  \n",
        "\n",
        "> **<시퀀스를 위한 합성곱>**\n",
        ">\n",
        ">시퀀스 모델링에서는 합성곱 연산으로만 시퀀스를 모델링하는 경향이 나타났었습니다. 디코딩 단계에서는 전치 합성곱 연산을 사용하는데 합성곱 모델을 사용하면 훈련 속도를 크게 높일 수 있습니다.  \n",
        "\n",
        "> **<필요한 것은 어텐션이 전부이다>**  \n",
        ">\n",
        ">또 다른 트렌드는 합성곱을 어텐션 메커니즘으로 바꾸는 것입니다. 특히 셀프 어텐션과 멀티헤드 어텐션으로 알려진 어텐션 메커니즘을 사용해 넓은 범위의 의존성을 감지합니다.  \n",
        "일반적으로 RNN과 CNN을 사용해서 모델을 만듭니다.  \n",
        "\n",
        "> **<전이 학습>**  \n",
        ">\n",
        ">전이 학습(Transfer Learning)은 한 작업에서 학습한 표현을 사용해서 다른 작업의 학습을 향상시키는 방법입니다. 최근 신경망과 딥러닝이 NLP 분야의 주요 기술로 자리 잡으면서 사전 훈련된 단어 벡터를 사용한 전이 학습 기법이 많이 사용됩니다. Redford 등의 논문과 Peters 등의 논문은 언어 모델링 작업에서 학습한 비지도 표현이 어떻게 다양한 NLP 작업(질의응답, 분류, 문장 유사도, 자연어 추론과 같은)에 도움이 될 수 있는지 보여줍니다.  \n",
        "\n",
        "이 외에도 ***강화 학습(Reinforcement Learning)*** 분야는 최근 대화 기반 작업에서 어느 정도 성공을 거두었습니다. 복잡한 자연어 추론 작업에 메모리와 지식 기반을 사용한 모델링이 산업계와 학계의 연구자들에게 큰 관심을 끄는 것으로 보입니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1prpCtvtXBTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HVreOhSwWAZM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}